import logging
from copy import deepcopy
from typing import Any, Iterable, ClassVar, Type
from pathlib import Path
from dataclasses import dataclass, field

import pandas as pd
import plotly.express as px

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from src.utils.iox import ProgramData
from src.utils.programming import EventHandler
from src.datamodels.variables import Variable
from src.datamodels.spaces import Space
from src.pman.datamodels.problems import GenerativeDesignProblem, ConditionalGenerativeDesignProblem, GenerativeModelingProblem

from src.mdev.utilities import data_container_row_index, MDevDataset, NormalizedVariable
from src.mdev.archspec import LayerSpecification, get_layer_spec_from_dct, ModuleLinkedSpecification
from src.mdev.logger import LOG_LEVEL_TRAINING


logger = logging.getLogger(__name__)


class GenerativeModel:

    MODEL_TYPE_ABBREVIATION: str = None
    MODEL_TYPE_NAME_LONG: str = None

    EVENT_TRAIN_START = 'train_start'
    EVENT_TRAIN_END = 'train_end'
    EVENT_TRAIN_EPOCH_COMPLETE = 'train_1_epoch'

    @dataclass
    class ModelSpecification(ProgramData):

        generate_space: Space
        """Space of variables generated by the model."""

        _data_type_str: ClassVar[str] = 'generative_model_specification'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = ['generate_space']
        _used_classes: ClassVar[list[Type['ProgramData']]] = [Space]

    @dataclass
    class ModelHyperparameters(ProgramData):

        n_generate_features: int
        """Number of features to generate"""

        _data_type_str: ClassVar[str] = 'generative_model_hyperparameters'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = ['n_generate_features']

    @dataclass
    class TrainingSettings(ProgramData):

        batch_size: int

        dataloader_drop_last: bool
        """Drop last incomplete batch in the dataloader if the batch size is less than specified."""
        dataloader_shuffle: bool

        optimizer_key: str
        """String key identifying an optimizer type"""
        optimizer_settings: dict[str, Any]
        """Arguments of the optimizer instance, will be expanded inside the optimizer instance constructor."""
        torch_manual_seed: int | float | None

        _data_type_str: ClassVar[str] = 'generative_model_training_settings'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = [
            'batch_size', 'dataloader_drop_last', 'dataloader_shuffle',
            'optimizer_key', 'optimizer_settings', 'torch_manual_seed'
        ]

    @dataclass
    class TrainingControls(ProgramData):

        autosave_period: int
        autosave_dir: str

        _data_type_str: ClassVar[str] = 'generative_model_training_controls'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = ['autosave_period', 'autosave_dir']

    @dataclass
    class TrainingStatus(ProgramData):

        current_train_epoch: int
        epochs_losses: dict

        train_dataset_id: str
        train_data_groups_means: list[list[float]]
        train_data_groups_stds: list[list[float]]

        torch_seed: float | int

        _data_type_str: ClassVar[str] = 'generative_model_training_status'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = ['current_train_epoch', 'epochs_losses', 'train_dataset_id', 'train_data_groups_means', 'train_data_groups_stds']

    @dataclass
    class ModelRecipe(ProgramData):

        id: str | int
        hyperparameters: 'GenerativeModel.ModelHyperparameters'
        specification: 'GenerativeModel.ModelSpecification'
        training_settings: 'GenerativeModel.TrainingSettings'

        _data_type_str: ClassVar[str] = 'generative_model_recipe'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = ['id', 'hyperparameters', 'training_settings', 'specification']
        _used_classes: ClassVar[list[Type['ProgramData']]] = []

        @classmethod
        def from_dict(cls, dct: dict) -> 'ProgramData':
            return super().from_dict(dct)

    ModelRecipe._used_classes = [ModelSpecification, ModelHyperparameters, TrainingSettings]
    # this is a bad, hacky way of doing this.

    @dataclass
    class ModelRecord:

        instance_id: str | int
        recipe: 'GenerativeModel.ModelRecipe'
        training_controls: 'GenerativeModel.TrainingControls'
        training_status: 'GenerativeModel.TrainingStatus'

    def __init__(self,
                 instance_id: str | int,
                 model_recipe: ModelRecipe,
                 training_controls: TrainingControls,
                 device: torch.device
                 ):

        self.instance_id = instance_id

        self.device = device

        self.recipe = model_recipe
        self.specification = self.recipe.specification
        self.hps = self.recipe.hyperparameters
        self.training_settings = self.recipe.training_settings

        self.training_controls = training_controls

        if self.training_settings.torch_manual_seed is not None:
            torch.manual_seed(self.training_settings.torch_manual_seed)
            self.torch_seed = self.training_settings.torch_manual_seed
        else:
            self.torch_seed = torch.seed()

        self._create_model()
        self._init_optimizer()

        self.event_handler = EventHandler(self.name)
        for event_name in [self.EVENT_TRAIN_START, self.EVENT_TRAIN_EPOCH_COMPLETE, self.EVENT_TRAIN_END]:
            self.event_handler.create_event(event_name)

        self.periodic_events: dict[int, list[str]] = {}

        self.current_train_epoch: int = 0
        """Index of current training epoch."""

        self.train_dataset_id = None
        """Name of the dataset used for training."""
        self.train_data_groups_means = None
        """Mean of the dataset used to train this model."""
        self.train_data_groups_stds = None
        """Standard deviation of the dataset used to train this model."""

        self.training_losses: dict[int, dict[str, float]] = {}
        """Training losses indexed by epoch number. For each epoch, stores a dictionary with keys of
        different types of losses mapped to loss values."""
        self.event_handler.subscribe(self.EVENT_TRAIN_EPOCH_COMPLETE, self._update_training_losses)

        self.add_periodic_save(epochs=self.training_controls.autosave_period)

    def __post_init__(self):
        pass

    @property
    def name(self) -> str:
        return f'{self.recipe.id}_{self.instance_id}'

    def set_mdev_dataset_groups(self, dataset: MDevDataset):
        """Set the "groups" in which the ``MDevDataset`` instance will release data. The model instance
        expects the different groups of dataset inputs to be grouped as specified, i.e. we do not
        distinguish between columns of different meanings by column indices etc. later on. Instead,
        we set groups here of different types of variables."""

        if not all(var in dataset.columns for var in self.specification.generate_space.variables):
            message = 'Training dataset is missing some expected columns from the generation space.\n'
            message += f'Missing columns: {", ".join([var.name for var in self.specification.generate_space.variables if var not in dataset.columns])}'
            raise KeyError(message)

        groups = [
            [data_container_row_index],
            self.specification.generate_space.variables,
        ]

        dataset.set_groups(groups)

    def _create_model(self):
        """Construct the model architecture."""
        pass

    def _init_optimizer(self):
        """Initialize the optimizer(s) used for training."""
        pass

    def generate(self, n_samples: int) -> torch.Tensor:
        """Returns ``n_samples`` samples generated by the model."""
        pass

    def _training_restart_checks(self, dataset: MDevDataset):

        if self.current_train_epoch != 0:
            if (dataset.data_container.id != self.train_dataset_id
                    or dataset.cols_means_grouped != self.train_data_groups_means
                    or dataset.cols_stds_grouped != self.train_data_groups_stds):
                message = f'This model was previously trained on a different dataset.'
                raise ValueError(message)

    def train(self, dataset: MDevDataset, epochs: int) -> None:
        """
        Train / fit the model to the provided data.

        :param dataset: Training data dataset.
        :param epochs: Number of epochs to train for
        """
        self.set_mdev_dataset_groups(dataset)
        self._training_restart_checks(dataset)

        self.train_dataset_id = dataset.data_container.id
        self.train_data_groups_means = dataset.cols_means_grouped
        self.train_data_groups_stds = dataset.cols_stds_grouped

        dataloader = DataLoader(dataset,
                                batch_size=self.training_settings.batch_size,
                                shuffle=self.training_settings.dataloader_shuffle,
                                drop_last=self.training_settings.dataloader_drop_last)

        self.activate_train_mode()
        self.event_handler.emit(self.EVENT_TRAIN_START)
        logger.info(f'Model "{self.name}" training is starting.')
        logger.info(f'Training directory: {self.training_controls.autosave_dir}')

        for epoch_index in range(self.current_train_epoch, self.current_train_epoch + epochs):
            self.current_train_epoch += 1

            self._train_epoch(epoch_index, dataloader)

        logger.info(f'End of training ({epochs} epochs)')
        self.event_handler.emit(self.EVENT_TRAIN_END)

    def activate_train_mode(self):
        raise NotImplementedError()

    def _train_epoch(self, epoch_index: int, dataloader: DataLoader):

        self._current_epoch_losses_sums: dict[str, float] = {}

        last_batch_train_data = None
        for batch_index, batch_data in enumerate(dataloader):
            self._train_batch(batch_index, batch_data)
            last_batch_train_data = batch_data

        # Record average losses in this epoch
        self._current_epoch_losses = {
            loss_type_name: loss_type_sum / len(dataloader)
            for loss_type_name, loss_type_sum in self._current_epoch_losses_sums.items()
        }

        losses_str = ' / '.join([f'{loss_type_name}={loss_value:.04e}' for loss_type_name, loss_value in self._current_epoch_losses.items()])
        logger.log(LOG_LEVEL_TRAINING, f'Epoch {epoch_index:03d}: {losses_str}')

        self._do_end_of_epoch_callbacks(epoch_index, self._current_epoch_losses, None, last_batch_train_data)

    def _train_batch(self, batch_index: int, batch_data: tuple[torch.Tensor, ...]):
        """Process one (mini)batch of training data and update the network using loss based on the batch."""

        # As per ``training_data_groups``
        batch_indices, batch_gvars = batch_data

        pass

    @dataclass
    class EpochReport:

        epoch_index: int
        epoch_losses: dict[str, float]
        """Losses calculated in the current epoch, indexed by name of the type of loss."""
        last_batch_train_data: Any
        last_batch_generated_data: Any

    def _do_end_of_epoch_callbacks(self,
                                   epoch_index: int,
                                   epoch_losses: dict[str, float],
                                   last_batch_train_data: torch.Tensor,
                                   last_batch_generated_data: torch.Tensor) -> None:

        epoch_report_data = self.EpochReport(epoch_index, epoch_losses, last_batch_train_data, last_batch_generated_data)

        event_names = [self.EVENT_TRAIN_EPOCH_COMPLETE]
        for epoch_period in self.periodic_events:
            if (epoch_index - 0 + 1) % epoch_period == 0:
                event_names += self.periodic_events[epoch_period]

        for event_name in event_names:
            self.event_handler.emit(event_name, epoch_report_data)

    def add_periodic_save(self, epochs: int) -> str:
        """Creates an event for saving the model every N epochs during training, and subscribes the ``save`` method
        to the created event.

        :param epochs: N in every N epochs to save at

        :return: Event name created for the autosave at every N epochs
        """
        event_name = f'save_{epochs}_epochs'
        self.create_event_for_nth_epoch(epochs, event_name=event_name)
        self.event_handler.subscribe(event_name, callback=self._callback_autosave)
        return event_name

    def get_training_status(self) -> TrainingStatus:
        return GenerativeModel.TrainingStatus(
            current_train_epoch=self.current_train_epoch,
            epochs_losses=self.training_losses,
            train_dataset_id=self.train_dataset_id,
            train_data_groups_means=self.train_data_groups_means,
            train_data_groups_stds=self.train_data_groups_stds,
            torch_seed=self.torch_seed
        )

    def _restore_from_training_status(self, training_status: TrainingStatus):
        self.current_train_epoch = training_status.current_train_epoch
        self.training_losses = training_status.epochs_losses
        self.train_dataset_id = training_status.train_dataset_id
        self.train_data_groups_means = training_status.train_data_groups_means
        self.train_data_groups_stds = training_status.train_data_groups_stds

    def get_record(self) -> ModelRecord:
        pass

    def _callback_autosave(self, *args, **kwargs):
        # Created to digest the default arguments sent by periodic callback
        self.save_record()

    def save_record(self, custom_suffix: str = None, *args, **kwargs):
        record = self.get_record()

        dir = Path(self.training_controls.autosave_dir)
        fn = f'{self.name}_e{record.training_status.current_train_epoch}'
        if custom_suffix is not None:
            fn += f'_{custom_suffix}'
        fn += '.pt'

        torch.save(record, dir / fn)
        logger.info('Model saved.')

    @classmethod
    def load(cls, fp: str | Path, device: torch.device) -> 'GenerativeModel':

        record: GenerativeModel.ModelRecord = torch.load(fp)

        model = cls(
            instance_id=record.instance_id,
            model_recipe=record.recipe,
            training_controls=record.training_controls,
            device=device
        )

        model._restore_from_training_status(record.training_status)

        return model

    def create_event_for_nth_epoch(self, n: int, event_name: str = None) -> str:
        """
        Create an event in the handler of the model for every Nth epoch. An event will be created with an
        automatically generated name, or with the provided name in ``event_name`` argument.

        :param n: Period, i.e. N in every Nth epoch
        :param event_name: Optional, custom event name

        :return: Name of the created event
        """

        if event_name is None:
            if n in self.periodic_events:
                event_name = self.periodic_events[n][0]
                message = f'An event for every {n}th epoch has already been created: "{event_name}"'
                raise ValueError(message)
            else:
                event_name = f'train_{n}_epochs'
                self.periodic_events[n] = [event_name]
                self.event_handler.create_event(event_name)
                return event_name
        else:
            if n in self.periodic_events and event_name in self.periodic_events[n]:
                message = f'An event "{event_name}" for every {n}th epoch has already been created.'
                raise ValueError(message)
            if n not in self.periodic_events:
                self.periodic_events[n] = []
            self.periodic_events[n].append(event_name)
            self.event_handler.create_event(event_name)
            return event_name

    def _update_training_losses(self, epoch_report: EpochReport, *args, **kwargs):
        self.training_losses[epoch_report.epoch_index] = epoch_report.epoch_losses

    def __repr__(self):
        return f'{self.name} (epoch {self.current_train_epoch} on training with {self.train_dataset_id})'



class ConditionalGenerativeModel(GenerativeModel):

    @dataclass
    class ModelSpecification(GenerativeModel.ModelSpecification):

        condition_space: Space

        _data_type_str: ClassVar[str] = 'conditional_generative_model_specification'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = GenerativeModel.ModelSpecification._save_fields + ['condition_space']

    @dataclass
    class ModelHyperparameters(GenerativeModel.ModelHyperparameters):

        n_condition_features: int
        """Number of conditioning features"""

        _data_type_str: ClassVar[str] = 'conditional_generative_model_hyperparameters'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = GenerativeModel.ModelHyperparameters._save_fields + ['n_condition_features']

    def __init__(self,
                 instance_id: str | int,
                 model_recipe: GenerativeModel.ModelRecipe,
                 training_controls: GenerativeModel.TrainingControls,
                 device: torch.device
                 ):

        super().__init__(instance_id, model_recipe, training_controls, device)
        self.specification = model_recipe.specification

    def set_mdev_dataset_groups(self, dataset: MDevDataset):
        """Set the "groups" in which the ``MDevDataset`` instance will release data. The model instance
        expects the different groups of dataset inputs to be grouped as specified, i.e. we do not
        distinguish between columns of different meanings by column indices etc. later on. Instead,
        we set groups here of different types of variables."""

        if not all(var in dataset.columns for var in self.specification.generate_space.variables + self.specification.condition_space.variables):
            message = 'Training dataset is missing some expected columns from generation or condition space.'
            raise KeyError(message)

        # Make the MDevDataset release each minibatch data in 3 groups: row_index, generate_vars, condition_vars
        groups = [
            [data_container_row_index],
            self.specification.generate_space.variables,
            self.specification.condition_space.variables
        ]

        dataset.set_groups(groups)

    def generate(self, n_samples: int, condition: torch.Tensor = None) -> torch.Tensor:
        pass


ConditionalGenerativeModel.ModelRecipe._used_classes += [ConditionalGenerativeModel.ModelSpecification,
                                                         ConditionalGenerativeModel.ModelHyperparameters]


class GenerativeModelOutputAdapter:

    def __init__(self,
                 model: GenerativeModel,
                 base_problem: GenerativeModelingProblem):
        """
        A helper class to process the outputs of a generative model. Namely,

        - unnormalizes generative model outputs, if they are normalized
        - returns only the generation outputs that are meaningful for the problem, e.g. removes padding etc. dummy generation outputs.

        :param model: Generative model to process outputs of.
        :param base_problem: **Base problem** that motivates the generative model, i.e. not the expanded, derived problem, e.g. one with
            padding variables
        """

        self.model = model
        self.base_problem = base_problem

    def unnormalize(self, model_generation_outputs: torch.Tensor) -> tuple[torch.Tensor, tuple[Variable, ...]]:
        """Based on the ``model.specification.generate_space`` definition, "unnormalizes" the columns in the
        generation output tensor if the variable represented by the column is a ``NormalizedVariable``."""
        outputs = model_generation_outputs

        # Variables meaningful for the problem
        generated_vars = self.model.specification.generate_space.variables

        returned_vars = []
        for col_index, var in enumerate(generated_vars):
            if isinstance(var, NormalizedVariable):
                outputs[:, col_index] = outputs[:, col_index] * var.normalize_std + var.normalize_mean
                returned_vars.append(var.base_variable)
            else:
                returned_vars.append(var)

        return outputs, tuple(returned_vars)

    def get_problem_generate_space_values(self, model_generation_outputs: torch.Tensor) -> list[dict[Variable, float]]:
        """Returns unnormalized values of variables in the ``problem.generate_space`` out of the raw tensor of
        potentially normalized values from the model."""

        unnormalized, unnormalized_vars = self.unnormalize(model_generation_outputs)
        unnormalized = unnormalized.tolist()

        problem_generate_space_vars = self.base_problem.generate_space.variables

        cols_vars_to_extract = []
        for unnormalized_col_index, unnormalized_var in enumerate(unnormalized_vars):
            if unnormalized_var in problem_generate_space_vars:
                cols_vars_to_extract.append((unnormalized_col_index, unnormalized_var))

        to_return = []
        for unnormalized_generation_output in unnormalized:
            to_return.append({var: unnormalized_generation_output[col_index] for col_index, var in cols_vars_to_extract})

        return to_return

    def generate_raw(self, n_samples: int = 1) -> torch.Tensor:
        raw_generation_output = self.model.generate(n_samples)
        return raw_generation_output

    def generate_useful(self, n_samples: int = 1) -> list[dict[Variable, float]]:
        """Get generation outputs for the provided conditions, return only the variables useful to the
        problem definition in unnormalized form."""

        raw_generation_output = self.generate_raw(n_samples)
        unnormalized_useful_vars = self.get_problem_generate_space_values(raw_generation_output)

        return unnormalized_useful_vars


class ConditionalGenerativeModelOutputAdapter(GenerativeModelOutputAdapter):

    def generate_raw(self, n_samples: int = 1, conditions: tuple[float, ...] = None) -> torch.Tensor:
        if conditions is None:
            message = 'Conditions must be provided.'
            raise ValueError(message)

        # Convert conditions into a 1D tensor
        conditions = torch.tensor(conditions, device=self.model.device).reshape(-1)

        # Normalize conditions the way model accepts them
        assert isinstance(self.model, ConditionalGenerativeModel)
        spec_condition_vars = self.model.specification.condition_space.variables

        if conditions.shape[0] != len(spec_condition_vars):
            message = (f'Provided number of conditions ({len(conditions.shape[1])}) '
                       f'do not match the number model accepts ({len(spec_condition_vars)})')
            raise ValueError(message)

        for c_index, spec_condition_var in enumerate(spec_condition_vars):
            if isinstance(spec_condition_var, NormalizedVariable):
                conditions[c_index] = (conditions[c_index] - spec_condition_var.normalize_mean) / spec_condition_var.normalize_std

        raw_generation_output = self.model.generate(n_samples, conditions)
        return raw_generation_output

    def generate_useful(self, n_samples: int = 1, conditions: tuple[float] = None) -> list[dict[Variable, float]]:
        """Get generation outputs for the provided conditions, return only the variables useful to the
        problem definition in unnormalized form."""
        if conditions is None:
            message = 'Conditions must be provided.'
            raise ValueError(message)

        raw_generation_output = self.generate_raw(n_samples, conditions)
        unnormalized_useful_vars = self.get_problem_generate_space_values(raw_generation_output)

        return unnormalized_useful_vars


class Validator:

    validation_loss_type = None

    @dataclass
    class LossHistory(ProgramData):

        loss_type: str
        epochs_losses: dict

        _data_type_str: ClassVar[str] = 'loss_history'
        _data_type_key: ClassVar[int] = 550

        _save_fields: ClassVar[list[str]] = ['loss_type', 'epochs_losses']
        _used_classes: ClassVar[list[Type['ProgramData']]] = []

    def __init__(self,
                 model: GenerativeModel):
        """Template for a validation loss calculator to integrate to generative model training."""

        self.model = model

        self.validation_losses: dict[int, float] = {}
        """Losses indexed by the epoch at which they are calculated."""

        self.current_min_val_loss = None
        self.current_min_val_loss_model: GenerativeModel = None
        """Copy of the model at the state where it achieved the minimum validation loss."""

        self.model.event_handler.subscribe(self.model.EVENT_TRAIN_END, self.callback_end_of_training)

    def callback_end_of_training(self, *args, **kwargs):

        # Save minimum validation loss model
        if self.current_min_val_loss_model is not None:
            self.current_min_val_loss_model.save_record(
                custom_suffix=f'min-val-loss-{self.validation_loss_type}'
            )

        # Save validation loss history
        fn = f'{self.model.name}_val-losses_e{self.model.current_train_epoch}.json'
        hist = self.LossHistory(self.validation_loss_type, self.validation_losses)
        hist.write(Path(self.model.training_controls.autosave_dir) / fn)

    def calculate_validation_loss(self, epoch_report: GenerativeModel.EpochReport):
        pass

    def _add_to_validation_losses(self, epoch_index: int, validation_loss: float):
        """Updates the ``validation_losses`` dictionary and the minimum validation loss trackers.
        Also saves a copy of the minimum validation loss model."""
        self.validation_losses[epoch_index] = validation_loss

        if (self.current_min_val_loss is None) or validation_loss < self.current_min_val_loss:
            self.current_min_val_loss = validation_loss
            # self.current_min_val_loss_model = deepcopy(self.model)


class TrainingProgressPlotter:

    KEY_EPOCH = 'Epoch'
    KEY_LOSS_TYPE = 'Loss Type'
    KEY_LOSS_VALUE = 'Loss'
    PREFIX_TRAINING_LOSS = 'Training'
    PREFIX_VALIDATION_LOSS = 'Validation'

    def __init__(self,
                 model: GenerativeModel,
                 validators: Iterable[Validator]):
        """
        Ready-to-use training progress plotter. Register multiple validator instances.

        :param model: Model instance being trained
        :param validators: Validator instances from which loss will be obtained, indexed by the name of the type of
        the loss given by the validator.
        """

        self.model = model
        self.validators = validators

    def generate_progress_plot(self, epoch_report: GenerativeModel.EpochReport):
        epoch_index = epoch_report.epoch_index

        df_rows = []

        # Add training losses
        for epoch, training_losses in self.model.training_losses.items():
            for loss_type, loss_value in training_losses.items():
                df_rows.append({self.KEY_EPOCH: epoch,
                                self.KEY_LOSS_TYPE: f'{self.PREFIX_TRAINING_LOSS}: {loss_type}',
                                self.KEY_LOSS_VALUE: loss_value})

        # Add validation losses
        for validator in self.validators:
            loss_type = validator.validation_loss_type

            df_rows += [{self.KEY_EPOCH: epoch,
                         self.KEY_LOSS_TYPE: f'{self.PREFIX_VALIDATION_LOSS}: {loss_type}',
                         self.KEY_LOSS_VALUE: loss}
                        for epoch, loss in validator.validation_losses.items()]

        df = pd.DataFrame(df_rows)

        fig = px.line(
            df,
            x=self.KEY_EPOCH,
            y=self.KEY_LOSS_VALUE,
            color=self.KEY_LOSS_TYPE,
            title=f'{self.model.name} ({self.model.MODEL_TYPE_ABBREVIATION}) at Epoch {epoch_index}',
            markers=True
        )

        fn = f'{self.model.name}_losses.html'
        fp = Path(self.model.training_controls.autosave_dir) / fn
        fp_old_temp = Path(self.model.training_controls.autosave_dir) / f'${fn}'

        try:
            if fp.exists():
                fp.rename(fp_old_temp)
        except PermissionError:
            return

        fig.write_html(str(fp))

        try:
            fp_old_temp.unlink(missing_ok=True)
        except PermissionError:
            return


if __name__ == '__main__':

    print(5)
